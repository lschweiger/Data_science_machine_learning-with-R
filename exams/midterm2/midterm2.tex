\documentclass[12pt]{article}

\include{preamble}

\title{Math 390.4 / 650.3 Spring 2018 \\ Midterm Examination Two}
\author{Professor Adam Kapelner}

\date{Monday, April 16, 2018}

\begin{document}
\maketitle

\noindent Full Name \line(1,0){410}

\thispagestyle{empty}

\section*{Code of Academic Integrity}

\footnotesize
Since the college is an academic community, its fundamental purpose is the pursuit of knowledge. Essential to the success of this educational mission is a commitment to the principles of academic integrity. Every member of the college community is responsible for upholding the highest standards of honesty at all times. Students, as members of the community, are also responsible for adhering to the principles and spirit of the following Code of Academic Integrity.

Activities that have the effect or intention of interfering with education, pursuit of knowledge, or fair evaluation of a student's performance are prohibited. Examples of such activities include but are not limited to the following definitions:

\paragraph{Cheating} Using or attempting to use unauthorized assistance, material, or study aids in examinations or other academic work or preventing, or attempting to prevent, another from using authorized assistance, material, or study aids. Example: using an unauthorized cheat sheet in a quiz or exam, altering a graded exam and resubmitting it for a better grade, etc.
\\

\noindent I acknowledge and agree to uphold this Code of Academic Integrity. \\

\begin{center}
\line(1,0){250} ~~~ \line(1,0){100}\\
~~~~~~~~~~~~~~~~~~~~~signature~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ date
\end{center}

\normalsize

\section*{Instructions}

This exam is 110 minutes and closed-book. You are allowed \textbf{one} page (front and back) of a \qu{cheat sheet.} You may use a graphing calculator of your choice. Please read the questions carefully. If the question reads \qu{compute,} this means the solution will be a number otherwise you can leave the answer in \textit{any} widely accepted mathematical notation which could be resolved to an exact or approximate number with the use of a computer. I advise you to skip problems marked \qu{[Extra Credit]} until you have finished the other questions on the exam, then loop back and plug in all the holes. I also advise you to use pencil. The exam is 100 points total plus extra credit. Partial credit will be granted for incomplete answers on most of the questions. \fbox{Box} in your final answers. Good luck!

\pagebreak



\problem This question is about concepts of OLS.

\benum

\subquestionwithpoints{4} Solve for $\c^\star$ where $B \in \reals^{n \times m}$ where $n > m$ and $B$ is full rank:

\beqn
\c^\star = \argmin_{\c \in \reals^{m}}\braces{\c^\top B^\top B \c}
\eeqn~\spc{5}


\subquestionwithpoints{3} Assume $\X \in \reals^{n \times (p+1)}$ where $n >> p + 1$ and $\X$ is full rank and its first column is $\onevec_n$. In terms of $\X, n, p$, (1) give an expression for the matrix $\H$ which represents the orthogonal projection matrix onto the column space of $X$, (2) indicate the dimension of the matrix $\H$ and (3) indicate the rank of the matrix $\H$. 

\beqn
\H &=& \hspace{400px}\\
&& \\
\dim\bracks{\H} &=& \\
&& \\
\rank{\H} &=& 
\eeqn



\subquestionwithpoints{8} Assume $\b$ is the least squares solution, $\yhat$ is the projection of $\y$ onto the column space of $\X$ defined in (b) via projection matrix $\H$ and $\e$ is the difference between the original vector and this projection. Simplify the following \textit{as best as possible} or indicate an \textit{illegal operation}.

\beqn
\yhat \cdot \e &=& \hspace{400px}\\
&& \\
\yhat + \e &=& \\
&& \\
\yhat \cdot \y &=& \hspace{400px}\\
&& \\
\y \cdot \b &=& \hspace{400px}\\
&& \\
\H \Ht \yhat &=& \hspace{400px}\\
&& \\
(\I - \H)^\top \yhat &=& \hspace{400px}\\
&& \\
\normsq{\y} - \normsq{\X \b} - \normsq{\y - \yhat} &=& \hspace{400px}\\
&& \\
\H \threevec{\ybar}{\vdots}{\ybar} &=& \hspace{400px}\\
&& \\
\H \bracks{\onevec_n~|~\x_{\cdot 4}~|~\x_{\cdot 9}} &=& \hspace{400px} \\
\eeqn

\subquestionwithpoints{6} Assume all notation from (b) and (c). Let $\X = \Q\R$, the Q-R decomposition. Prove that $\b$ in the following expression is the standard least squares solution. Show all steps explicitly for full credit.


\beqn
\R \b = \Q^\top \y \hspace{400px}
\eeqn~\spc{10}

\subquestionwithpoints{9} Assume all notation means the same as in the previous questions. Now, let $\X_{\text{aug}} := \bracks{\X ~|~\x_{\text{junk}}}$ where $\x_{\text{junk}}$ is a $n \times 1$ vector whose entries are all $\iid \stdnormnot$. Let the subscript \qu{aug} refer to all quantities of the OLS solution using $\X_{\text{aug}}$ instead of $\X$. Circle the following statement(s) that are \textit{always} true.

\begin{enumerate}[i)]
\item $\normsq{\e} < \normsq{\e_{\text{aug}}}$
\item $\normsq{\e} > \normsq{\e_{\text{aug}}}$
\item $\normsq{\yhat} < \normsq{\yhat_{\text{aug}}}$
\item $\normsq{\yhat} > \normsq{\yhat_{\text{aug}}}$
\item $\normsq{\y} < \normsq{\y_{\text{aug}}}$
\item $\normsq{\y} > \normsq{\y_{\text{aug}}}$
\item $\normsq{\b} < \normsq{\b_{\text{aug}}}$
\item $\normsq{\b} > \normsq{\b_{\text{aug}}}$
\item $b_{\text{junk}} \approx 0$
\item $R^2 < R^2_{\text{aug}}$
\item $R^2 > R^2_{\text{aug}}$
\item $\normsq{\y} < \normsq{\y_{\text{aug}}}$
\item $\normsq{\y} > \normsq{\y_{\text{aug}}}$
\item $\rank{\H} > \rank{\H_{\text{aug}}}$
\item $\rank{\H} < \rank{\H_{\text{aug}}}$
\item $x_{\text{junk}} \in \colsp{\X_{\text{aug}}}$
\item $\yhat \in \colsp{\X_{\text{aug}}}$
\item $\yhat_{\text{aug}} \in \colsp{\X_{\text{aug}}}$
\end{enumerate}

\subquestionwithpoints{4} Assume $\b$ is now the least absolute cube solution (not the least squares solution). Simplify the following \textit{as best as possible} or indicate an \textit{illegal operation}.

\beqn
\yhat \cdot \e &=& \hspace{400px}\\~\\
&& \\
\yhat + \e &=& \\~\\
\eeqn

\eenum
\pagebreak

\problem This question is about the concept of model validation and the strategy we discussed in class.

\benum

\subquestionwithpoints{6} Let's say we divide scramble the rows of $\mathbb{D}$ then create a partition 

\beqn
\mathbb{D} = \threevec{\mathbb{D}_{\text{train}}}{\text{------}}{\mathbb{D}_{\text{test}}}
\eeqn

\noindent in a 4:1 ratio train : test (in number of rows). We then fit $g_1 = \mathcal{A}(\mathcal{H}, \mathbb{D}_{\text{train}})$, $g_2 = \mathcal{A}(\mathcal{H}, \mathbb{D}_{\text{test}})$ and $g_{\text{final}} = \mathcal{A}(\mathcal{H}, \mathbb{D})$. Which of the following statement(s) can be employed as a means of \textit{honest} model validation?


\begin{enumerate}[i)]
\item Comparing $g_1(\X_{\text{train}})$ to $\y_{\text{train}}$
\item Comparing $g_1(\X_{\text{train}})$ to $\y_{\text{test}}$
\item Comparing $g_1(\X_{\text{test}})$ to $\y_{\text{train}}$
\item Comparing $g_1(\X_{\text{test}})$ to $\y_{\text{test}}$
\item Comparing $g_2(\X_{\text{train}})$ to $\y_{\text{train}}$
\item Comparing $g_2(\X_{\text{train}})$ to $\y_{\text{test}}$
\item Comparing $g_2(\X_{\text{test}})$ to $\y_{\text{train}}$
\item Comparing $g_2(\X_{\text{test}})$ to $\y_{\text{test}}$
\item Comparing $g_{\text{final}}(\X_{\text{train}})$ to $\y_{\text{train}}$
\item Comparing $g_{\text{final}}(\X_{\text{train}})$ to $\y_{\text{test}}$
\item Comparing $g_{\text{final}}(\X_{\text{test}})$ to $\y_{\text{train}}$
\item Comparing $g_{\text{final}}(\X_{\text{test}})$ to $\y_{\text{test}}$
\end{enumerate}

\eenum

\pagebreak

\problem This question is about \qu{non-linear} linear modeling. Consider the following data:


\begin{figure}[htp]
\centering
\includegraphics[width=5.5in]{nonlinear}
\end{figure}

\noindent Imagine if $\mathbb{D}$ consisted of the subset of the data pictured above where $\mathcal{X} = \braces{x : x \geq 0}$ i.e. no triangle points are part of the historical data. Consider $\mathcal{A} = $ OLS and the following model candidate sets:

\beqn
\mathcal{H}_1 &=& \braces{w_0 + w_1 x} \\
\mathcal{H}_2 &=& \braces{w_0 + w_1 x^2} \\
\eeqn

\benum

\subquestionwithpoints{3} Which model candidate set would be better for building a model $g$ using $\mathbb{D}$ whose goal is to predict in $\mathcal{X} = \braces{0, 3}$?

\begin{enumerate}[i)]
\item $\mathcal{H}_1$
\item $\mathcal{H}_2$
\item not enough information to tell
\end{enumerate}

\subquestionwithpoints{3} Which model candidate set would be better for building a model $g$ using $\mathbb{D}$ whose goal is to predict in $\mathcal{X} = \braces{-3, 3}$?

\begin{enumerate}[i)]
\item $\mathcal{H}_1$
\item $\mathcal{H}_2$
\item not enough information to tell
\end{enumerate}
\pagebreak

\subquestionwithpoints{3} Which model candidate set would be better for building a model $g$ using $\mathbb{D}$ whose goal is to predict in $\mathcal{X} = \reals$?

\begin{enumerate}[i)]
\item $\mathcal{H}_1$
\item $\mathcal{H}_2$
\item not enough information to tell
\end{enumerate}

\eenum


\problem We continue with \qu{non-linear} linear modeling. We will consider a similar-looking dataset as in the previous problem but the situation will be totally different. Below the response $y$ is plotted by predictor $x$. However there is a second dummy predictor $z$ which is pictured below as well. If $z=1$, the illustration displays a circle and if $z=0$, the illustration displays a triangle. The entire $\mathbb{D}$ is plotted below.


\begin{figure}[htp]
\centering
\includegraphics[width=5.3in]{nonlinear}
\end{figure}

Consider $\mathcal{A} = $ OLS and the following model candidate sets:

\beqn
\mathcal{H}_1 &=& \braces{w_0 + w_1 x} \\
\mathcal{H}_2 &=& \braces{w_0 + w_1 z} \\
\mathcal{H}_3 &=& \braces{w_0 + w_1 x^2} \\
\mathcal{H}_4 &=& \braces{w_0 + w_1 x + w_2 z + w_3 xz} \\
\eeqn

\benum

\subquestionwithpoints{3} Which model candidate set would be better for building a model $g$?

\begin{enumerate}[i)]
\item $\mathcal{H}_1$
\item $\mathcal{H}_2$
\item $\mathcal{H}_3$
\item $\mathcal{H}_4$
\item not enough information to tell
\end{enumerate}


\subquestionwithpoints{6} Regardless of your answer in (a), assume $\mathcal{H}_4$ was employed. Estimate $\b$ as best as you can.\spc{9}

\eenum

\problem This question is about general concepts of modeling including under/overfitting.

\benum

\subquestionwithpoints{6} Assume a general $\mathbb{D}$, $\mathcal{A}$ and $\mathcal{H}$ and $\mathcal{Y} \subset \reals$.  In the graph below, (1) draw the relationship between in-sample error and model complexity, (2) draw the relationship between out-of-sample error and model complexity, then (3) indicate the region of underfitting and (4) indicate the region of overfitting.  \spc{-0.5}


\begin{figure}[htp]
\centering
\includegraphics[width=6.5in]{complexity}
\end{figure}

\pagebreak

\subquestionwithpoints{6} Assume a general phenomenon where you're given $\mathbb{D}$ and $\mathcal{Y} \subset \reals$ and $\mathcal{A}$ and corresponds to a least squares minimization for and a simple model space $\mathcal{H}$ with 10 parameters. Assume $\epsilon$ is non-zero. Now, (1) draw the relationship between in-sample error and $n$, the number of data points in $\mathbb{D}$, (2) draw the relationship between out-of-sample error and $n$.  \spc{-0.5}


\begin{figure}[htp]
\centering
\includegraphics[width=5.35in]{n_increasing}
\end{figure}


\subquestionwithpoints{3} [Extra credit] Assume the same setup as in (b) but now the model space $\mathcal{H}$ is complex with 100 parameters. Now, (1) draw the relationship between in-sample error and $n$, the number of data points in $\mathbb{D}$, (2) draw the relationship between out-of-sample error and $n$. Make sure to indicate clearly how the relationships differ here from the relationships you drew in (b).  \spc{-0.5}


\begin{figure}[htp]
\centering
\includegraphics[width=5.35in]{n_increasing}
\end{figure}


\subquestionwithpoints{6} Consider the plot below. \spc{-0.5}


\begin{figure}[htp]
\centering
\includegraphics[width=5.5in]{sine}
\end{figure}

Which one(s) of the following statement(s) are most likely true?

\begin{enumerate}[i)]
\item the predictor $x$ and the response $y$ are correlated
\item the predictor $x$ and the response $y$ are associated
\item $s_{xy}$ will be approximately zero
\item $s_{xy}$ will be exactly zero
\item $r$ will be approximately zero
\item $r$ will be exactly zero
\item $\delta = 0$
\item $f(x) = 0$
\item the random variable $X$ (that generated the realizations of $x$ above) and the random variable $Y$ (that generated the $\iid$ realizations of $y$) are dependent
\item the random variable $X$ (that generated the realizations of $x$ above) and the random variable $Y$ (that generated the $\iid$ realizations of $y$) are independent
\item this data is only of theoretical interest and can never be found in the real world
\item a linear model with polynomial terms will take many degrees of freedom to fit well
\item a model with a intelligently selected $\mathcal{H}$ can be fit with very few degrees of freedom
\item this data can \textit{only} be fit if one uses three splits of $\mathbb{D}$ --- one for training, one for selection and one for testing
\end{enumerate}

\eenum
\pagebreak



\problem Below are some questions on the practice topics we studied. We first load the diamonds data and we remind ourselves of the response (\texttt{price}) and the 9 features:


\begin{lstlisting}
> pacman::p_load(ggplot2)
> data(diamonds)
> diamonds$cut = factor(as.character(diamonds$cut))
> diamonds$color = factor(as.character(diamonds$color))
> diamonds$clarity = factor(as.character(diamonds$clarity))
> summary(diamonds)
    carat               cut        color        clarity     
 Min.   :0.2000   Fair     : 1610   D: 6775   SI1    :13065  
 1st Qu.:0.4000   Good     : 4906   E: 9797   VS2    :12258  
 Median :0.7000   Ideal    :21551   F: 9542   SI2    : 9194  
 Mean   :0.7979   Premium  :13791   G:11292   VS1    : 8171  
 3rd Qu.:1.0400   Very Good:12082   H: 8304   VVS2   : 5066  
 Max.   :5.0100                     I: 5422   VVS1   : 3655  
                                    J: 2808   (Other): 2531  
     depth           table           price             x         
 Min.   :43.00   Min.   :43.00   Min.   :  326   Min.   : 0.000  
 1st Qu.:61.00   1st Qu.:56.00   1st Qu.:  950   1st Qu.: 4.710  
 Median :61.80   Median :57.00   Median : 2401   Median : 5.700  
 Mean   :61.75   Mean   :57.46   Mean   : 3933   Mean   : 5.731  
 3rd Qu.:62.50   3rd Qu.:59.00   3rd Qu.: 5324   3rd Qu.: 6.540  
 Max.   :79.00   Max.   :95.00   Max.   :18823   Max.   :10.740  
                                                                 
       y                z         
 Min.   : 0.000   Min.   : 0.000  
 1st Qu.: 4.720   1st Qu.: 2.910  
 Median : 5.710   Median : 3.530  
 Mean   : 5.735   Mean   : 3.539  
 3rd Qu.: 6.540   3rd Qu.: 4.040  
 Max.   :58.900   Max.   :31.800 
\end{lstlisting}
\vspace{-1cm}


\benum

\subquestionwithpoints{4} As best as you can, illustrate the output of the following code. Make sure you label axes and provide some tick marks.

\begin{lstlisting}
> ggplot(diamonds) + 
     geom_density(aes(carat))
\end{lstlisting}~\spc{6}


\subquestionwithpoints{4} We now run an anova model as follows:

\begin{lstlisting}
> anova_mod = lm(price ~ cut, diamonds)
\end{lstlisting}
\vspace{-0.8cm}

and below are the $\b$ and RMSE:

\begin{lstlisting}
> coef(anova_mod)
 (Intercept)      cutGood     cutIdeal   cutPremium cutVery Good 
   4358.7578    -429.8933    -901.2158     225.4999    -376.9979 
> summary(anova_mod)$sigma
[1] 3963.847
\end{lstlisting}
\vspace{-0.8cm}

The first six entries of the variable \texttt{cut} are:

\begin{lstlisting}
> head(diamonds$cut)
[1] Ideal     Premium   Good      Premium   Good      Very Good
Levels: Fair Good Ideal Premium Very Good
\end{lstlisting}
\vspace{-0.8cm}

Provide below the first six rows of the model matrix $\X$ for the model \texttt{price $\sim$ cut}.\spc{5}

\subquestionwithpoints{3} [Extra credit] Given the model and the results in (b), illustrate as best as you can the result of the following code. Credit will only be given to near perfect renditions.

\begin{lstlisting}
> ggplot(diamonds) + 
     geom_boxplot(aes(x = cut, y = price))
\end{lstlisting}~\spc{6}

\subquestionwithpoints{6} The first six entries of carat are 

\begin{lstlisting}
> head(diamonds$carat)
[1] 0.23 0.21 0.23 0.29 0.31 0.24
\end{lstlisting}
\vspace{-0.8cm}

Illustrate the result of the following code:

\begin{lstlisting}
> head(model.matrix(price ~ carat * cut, diamonds))
\end{lstlisting}~\spc{5}


\subquestionwithpoints{6} Consider $\mathcal{A} = $ OLS and the following models explaining diamond \texttt{price}:


\begin{enumerate}[1.]
\item a 4-degree polynomial of carat
\item all raw features
\item all features interacted with carat
\item all interactions
\end{enumerate}

Write code below to fit these four models and save them as \texttt{mod\_1}, \texttt{mod\_2}, \texttt{mod\_3}, \texttt{mod\_4}. \spc{6}

\subquestionwithpoints{4} If $R^2$ was employed to select the \qu{best} model of the four in (d), what would be the result? That is, which model would it declare the winner?\spc{0.5}

\subquestionwithpoints{5} [Extra credit] Write code below that will select the \qu{best} model of the four in (d) as measured by future predictive performance.

\eenum


\end{document}
