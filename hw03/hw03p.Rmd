---
title: "HW03p"
author: "Launy Schweiger"
date: "April 13, 2018"
output: pdf_document
---

```{r setup, cache = F}
#knitr::opts_chunk$set(error = TRUE) #this allows errors to be printed into the PDF
```

1. Load pacakge `ggplot2` below using `pacman`.

```{r}
pacman::p_load(ggplot2)
diamonds$cut=factor(as.character(diamonds$cut))
diamonds$clarity=factor(as.character(diamonds$clarity))
diamonds$color=factor(as.character(diamonds$color))
```

The dataset `diamonds` is in the namespace now as it was loaded with the `ggplot2` package. Run the following code and write about the dataset below.
It is a data set containing information about different diamonds attributes such as quality of cut, carat color, size, and the price
```{r}
?diamonds
str(diamonds)
```

What is $n$, $p$, what do the features mean, what is the most likely response metric and why?

n= 53940
p=10
price since that is how we determine if the diamonds is "good" or not.

Regardless of what you wrote above, the variable `price` will be the response variable going forward. 

Use `ggplot` to look at the univariate distributions of *all* predictors. Make sure you handle categorical predictors differently from continuous predictors.

```{r}
ggplot(data=diamonds)+ geom_bar(mapping=aes(x=cut))
ggplot(data=diamonds)+ geom_bar(mapping=aes(x=color))
ggplot(data=diamonds)+ geom_bar(mapping=aes(x=clarity))
ggplot(data=diamonds)+ geom_density(mapping=aes(x=carat))
ggplot(data=diamonds)+ geom_density(mapping=aes(x=depth))
ggplot(data=diamonds)+ geom_density(mapping=aes(x=table))
ggplot(data=diamonds)+ geom_density(mapping=aes(x=price))
ggplot(data=diamonds)+ geom_density(mapping=aes(x=x))
ggplot(data=diamonds)+ geom_density(mapping=aes(x=y))
ggplot(data=diamonds)+ geom_density(mapping=aes(x=z))
```

Use `ggplot` to look at the bivariate distributions of the response versus *all* predictors. Make sure you handle categorical predictors differently from continuous predictors. This time employ a for loop when an logic that handles the predictor type.

```{r}
plot=ggplot(data=diamonds)
for(i in seq(1,10)){
  if((names(diamonds)[i])=="price"){next}
    if(is.numeric(diamonds[[i]])){g=(plot+geom_bin2d(mapping=aes(x=diamonds[[i]],y=price))+xlab((names(diamonds))[i]))
    print(g)}
    if(is.factor(diamonds[[i]])){g=(plot+geom_boxplot(mapping=aes(x=diamonds[[i]],y=price))+xlab((names(diamonds))[i]))
    print(g)}
  }
```

Does depth appear to be mostly independent of price?

Yes

Look at depth vs price by predictors cut (using faceting) and color (via different colors).

```{r}
ggplot(data=diamonds)+geom_point(mapping=aes(x=z,y=price,col=color))+facet_grid(. ~cut)+ggtitle("Depth vs Price")+xlab("Depth")+xlim(0,10)
```


Does diamond color appear to be independent of diamond depth?

Yes

Does diamond cut appear to be independent of diamond depth?

no

Do these plots allow you to assess well if diamond cut is independent of diamond price? Yes / no

No

We never discussed in class bivariate plotting if both variables were categorical. Use the geometry "jitter" to visualize color vs clarity. visualize price using different colors. Use a small sized dot.

```{r}
ggplot(diamonds,aes(x=color,y=clarity,col=price))+geom_point()+geom_jitter(size=.3)
```

Does diamond clarity appear to be mostly independent of diamond color?

yes

2. Use `lm` to run a least squares linear regression using depth to explain price. 

```{r}
dp=lm(diamonds$price~diamonds$depth)
```

What is $b$, $R^2$ and the RMSE? What was the standard error of price originally? 


```{r}
b=summary(dp)$coef
b
r=summary(dp)$r.squared
r
rmse=summary(dp)$sigma
rmse
se=sd(diamonds$price)
se
```

Are these metrics expected given the appropriate or relevant visualization(s) above?

yes 

Use `lm` to run a least squares linear regression using carat to explain price. 

```{r}
cp=lm(diamonds$price~diamonds$carat)
```

What is $b$, $R^2$ and the RMSE? What was the standard error of price originally? 

```{r}
b=summary(cp)$coef
b
r=summary(cp)$r.squared
r
rmse=summary(cp)$sigma
rmse
se=sd(diamonds$price)
se
```

Are these metrics expected given the appropriate or relevant visualization(s) above?

yes

3. Use `lm` to run a least squares anova model using color to explain price. 

```{r}
anova_lm=lm(price~0+color,diamonds)
```

What is $b$, $R^2$ and the RMSE? What was the standard error of price originally? 

```{r}
b=summary(anova_lm)$coef
b
r=summary(anova_lm)$r.squared
r
rmse=summary(anova_lm)$sigma
rmse
se=sd(diamonds$price)
```

Are these metrics expected given the appropriate or relevant visualization(s) above?

yes

Our model only included one feature - why are there more than two estimates in $b$?

multiple factors aka each different color has its own estimate

Verify that the least squares linear model fit gives the sample averages of each price given color combination. Make sure to factor in the intercept here.

```{r}
mean(diamonds$price[diamonds$color=="D"])
mean(diamonds$price[diamonds$color=="E"])
mean(diamonds$price[diamonds$color=="F"])
mean(diamonds$price[diamonds$color=="G"])
mean(diamonds$price[diamonds$color=="H"])
mean(diamonds$price[diamonds$color=="I"])
mean(diamonds$price[diamonds$color=="J"])

```

Fit a new model without the intercept and verify the sample averages of each colors' prices *directly* from the entries of vector $b$.

```{r}
new=lm(price~0+color,diamonds)
new$coefficients
D=(subset(diamonds,subset = diamonds$color=="D"))
mean(D$price)
E=(subset(diamonds,subset = diamonds$color=="E"))
mean(E$price)
F=(subset(diamonds,subset = diamonds$color=="F"))
mean(F$price)
G=(subset(diamonds,subset = diamonds$color=="G"))
mean(G$price)
H=(subset(diamonds,subset = diamonds$color=="H"))
mean(H$price)
I=(subset(diamonds,subset = diamonds$color=="I"))
mean(I$price)
J=(subset(diamonds,subset = diamonds$color=="J"))
mean(J$price)
```

What would extrapolation look like in this model? We never covered this in class explicitly.

nothing since color is catagorical 

4. Use `lm` to run a least squares linear regression using all available features to explain diamond price. 

```{r}
all=lm(price~.,diamonds)
all
```

What is $b$, $R^2$ and the RMSE? Also - provide an approximate 95% interval for predictions using the empirical rule. 

```{r}
b=all$coef
b
r=summary(all)$r.squared
r
rmse=summary(all)$sigma
rmse
ci=2*rmse
ci
```


Interpret all entries in the vector $b$.
one unit increase of carat will increase 11256
one unit increase of depth will decrease -63
one unit increase of table will decrease -26
compared to the fair cut, cutgood  will be 579 more on average
compared to the fair cut, cut ideal  will be 832 more on average
compared to the fair cut, cut premium  will be 762 more on average
compared to the fair cut, cut very good  will be 726 more on average
compared to the color D, E  will be -209 less on average
compared to the color D, F  will be -272 less on average
compared to the color D, G  will be -482 less on average
compared to the color D, H  will be -980 less on average
compared to the color D, I  will be -1466 less on average
compared to the color D, J  will be -2369 less on average
compared to the clarity I1, IF  will be 5345 more on average
compared to the clarity I1, SI1  will be 3665 more on average
compared to the clarity I1, SI2 will be 2702 more on average
compared to the clarity I1, VS1  will be 4578 more on average
compared to the clarity I1, VS2 will be 4267 more on average
compared to the clarity I1, VVS1 will be 5007 more on average
compared to the clarity I1, VVS2  will be 4950 more on average


Are these metrics expected given the appropriate or relevant visualization(s) above? Can you tell from the visualizations?

yes

Comment on why $R^2$ is high. Think theoretically about diamonds and what you know about them.

It is high since we are explaining using many features and sub features that can explain the price of a dimaond

Do you think you overfit? Comment on why or why not but do not do any numerical testing or coding.

no, since we have to many features and sub features that explain all the differences in price.

Create a visualization that shows the "original residuals" (i.e. the prices minus the average price) and the model residuals.

```{r}
reds=all$residuals
null_r=(diamonds$price-mean(diamonds$price))
ggplot(data=diamonds,aes(reds))+geom_density(col = "grey", fill = "red", alpha = 0.4)+geom_density(mapping=aes(null_r),col = "grey", fill = "darkgreen", alpha = 0.4)
```


5. Reference your visualizations above. Does price vs. carat appear linear?

no looks more exponential

Upgrade your model in #4 to use one polynomial term for carat.

```{r}
poly1=lm(diamonds$price~.+I(diamonds$carat^2),data=diamonds)
```

What is $b$, $R^2$ and the RMSE? 

```{r}
poly1$coefficients
r=summary(poly1)$r.squared
r
rmse=summary(poly1)$sigma
rmse
```

Interpret each element in $b$ just like previously. You can copy most of the text from the previous question but be careful. There is one tricky thing to explain.

one unit increase of carat will increase 16114
one unit increase of depth will decrease -116
one unit increase of table will decrease -36
compared to the fair cut, cutgood  will be 538 more on average
compared to the fair cut, cut ideal  will be 807 more on average
compared to the fair cut, cut premium  will be 747 more on average
compared to the fair cut, cut very good  will be 678 more on average
compared to the color D, E  will be -209 less on average
compared to the color D, F  will be -284 less on average
compared to the color D, G  will be -496 less on average
compared to the color D, H  will be -997 less on average
compared to the color D, I  will be -1469 less on average
compared to the color D, J  will be -2357 less on average
compared to the clarity I1, IF  will be 5243 more on average
compared to the clarity I1, SI1  will be 3565 more on average
compared to the clarity I1, SI2 will be 2605 more on average
compared to the clarity I1, VS1  will be 4475 more on average
compared to the clarity I1, VS2 will be 4163 more on average
compared to the clarity I1, VVS1 will be 5904 more on average
compared to the clarity I1, VVS2  will be 4843 more on average
on average $carat^2$ will be -1028 since after certain point our polynomial will start to give inaccurate responses 

Is this an improvement over the model in #4? Yes/no and why.

yes $r^2$ went up and rmse went down

Define a function $g$ that makes predictions given a vector of the same features in $\mathbb{D}$.

```{r}
X=model.matrix(poly1,diamonds)
b=coef(poly1)

```

The inputs should be in following form g(c(number,cut number, color number, clarity number,x number, y number, z number, depth number,table number))
where cut number is 1=good, 2=very good, 3=premium , 4=ideal
      color number is 1=E,2=F,3=G,4=H,5=I,6=J
      clarity number is 1=SI1, 2=SI2,3=VS1,4=VS2,5=VSS1,6=VVS2

```{r}
g=function(xnew){
  if(xnew[2]==(1|2|3|4)){next} else{return(NULL)}
  if(xnew[3]==(1|2|3|4|5|6)){next}else{return(NULL)}
  if(xnew[4]==(1|2|3|4|5|6)){next} else{return(NULL)}
  b=coef(poly1)
    cuts=paste("cut",xnew[2],sep="")
    col=paste("color",xnew[3],sep="")
    cla=paste("clarity",xnew[4],sep="")
    b[cla]
 sum=(b["(Intercept)"]+b[2]*xnew[1]+b[cuts]+b[col]+b[cla]+b[20]*xnew[5]+b[21]*xnew[6]+b[22]*xnew[7]+b[23]*xnew[8]+b[24]*xnew[9]+b[25]*xnew[10])
 sum
}
```
cannot get function to work proply, dont know why?

6. Use `lm` to run a least squares linear regression using a polynomial of color of degree 2 to explain price.  

```{r error = TRUE}
lm(diamonds$price ~ poly(diamonds$color, 2, raw = TRUE))
```

Why did this throw an error?

color is not a numeric vector but made of discrete factors

7. Redo the model fit in #4 without using `lm` but using the matrix algebra we learned about in class. This is hard and requires many lines, but it's all in the notes.

```{r}
y=diamonds$price
reglm=lm(price~.,data=diamonds)
X=model.matrix(lm(price~.,data=diamonds),diamonds)
b = solve(t(X)%*%X)%*%t(X)%*%y
yhat=X%*%b
e=y-yhat
SSE = t(e) %*% e
p=(nrow(X)-(ncol(X)))
MSE = (1 / p) * SSE
RMSE = sqrt(MSE)
s_sq_y = var(y)
s_sq_e = var(e)
Rsq = (s_sq_y - s_sq_e) / s_sq_y
```

What is $b$, $R^2$ and the RMSE? 

```{r}
RMSE
Rsq
b
```

Are they the same as in #4?
yes

Redo the model fit using matrix algebra by projecting onto an orthonormal basis for the predictor space $Q$ and the Gram-Schmidt "remainder" matrix $R$. Formulas are in the notes. Verify $b$ is the same.

```{r}
indices = sample(1 : nrow(X), 2000)
X1 = X[indices, ]
y1 = y[indices]
rm(indices)
qrX = qr(X1)
Q = qr.Q(qrX)
R = qr.R(qrX)
#X1=Q%*%R
yhat_via_Q = Q%*%t(Q)%*%y1
head(yhat_via_Q)
bq=solve(R)%*%t(Q)%*%y1
bq
```

Generate the vectors $\hat{y}$, $e$ and the hat matrix $H$.

```{r}
XtX = t(X1) %*% X1
XtXinv = solve(XtX)
H = X1 %*% XtXinv %*% t(X1)
yhat = H %*% yhat_via_Q
head(yhat)
yhat = H %*% y1
e = y1 - yhat
e_q=y1-yhat_via_Q
```

In one line each, verify that 
(a) $\hat{y}$ and $e$ sum to the vector $y$ (the prices in the original dataframe), 
(b) $\hat{y}$ and $e$ are orthogonal 
(c) $e$ projected onto the column space of $X$ gets annhilated, 
(d) $\hat{y}$ projected onto the column space of $X$ is unaffected, 
(e) $\hat{y}$ projected onto the orthogonal complement of the column space of $X$ is annhilated
(f) the sum of squares residuals plus the sum of squares model equal the original (total) sum of squares

```{r}
pacman::p_load(testthat)
expect_equal(as.vector(e+yhat), y1) #(a)
expect_equal(sum(t(yhat_via_Q) %*% e_q),0,tol=1e-4) #(b)
expect_equal(sum(H %*% e_q),0) #(c)
expect_equal (as.vector(t(yhat_via_Q)%*%H),as.vector(t(yhat_via_Q)))#(d)
expect_equal(sum(t(yhat_via_Q)%*%e_q), 0 , tol = 1e-4) #(e)
y1bar=mean(y1)
SSR=sum((yhat_via_Q-y1bar)^2)
SSE=sum(e_q^2)
SST=sum((y1-y1bar)^2)
expect_equal(SSR + SSE,SST) # (f)
```

8. Fit a linear least squares model for price using all interactions and also 5-degree polynomials for all continuous predictors.

```{r}
#mod8=lm(price~.*.+I(carat^5)+I(x^5)+I(y^5)+I(z^5)*.+I(depth^5)+I(table^5),diamonds)
mod8p=lm(price~.*.+poly(carat,5,raw=TRUE)+poly(x,5,raw=TRUE)+poly(y,5,raw=TRUE)+poly(z,5,raw=TRUE)+poly(depth,5,raw=TRUE)+poly(table,5,raw=TRUE),diamonds)
```

Report $R^2$, RMSE, the standard error of the residuals ($s_e$) but you do not need to report $b$.

```{r}
summary(mod8p)$r.squared
summary(mod8p)$sigma

sd(mod8p$residuals)
```

Create an illustration of $y$ vs. $\hat{y}$.

```{r}
y8=diamonds$price
yhat8=as.vector(y-resid(mod8p))
ggplot(data=diamonds,aes(y8))+geom_density(fill="orchid2",alpha=.8)+geom_density(aes(yhat8),fill="green",alpha=.3)+xlab("y vs yhat")
```

How many diamonds have predictions that are wrong by \$1,000 or more ?

```{r}
  count8=0
for(i in 1:nrow(diamonds)){
  if((y8[i]-yhat8[i])>=1000 | y8[i]-yhat8[i]<=-1000)
    count8=count8+1
}
count8
```

$R^2$ now is very high and very impressive. But is RMSE impressive? Think like someone who is actually using this model to e.g. purchase diamonds.

no since the range is about 2500 per guess since the mean is about 3900

What is the degrees of freedom in this model?

```{r}
#length(mod8$coefficients)
length(mod8p$coefficients)
```

Do you think $g$ is close to $h^*$ in this model? Yes / no and why?

no, we have a large $h^*$ to include polynimals and interaction terms.

Do you think $g$ is close to $f$ in this model? Yes / no and why?

yes since they overlap all most ever where

What more degrees of freedom can you add to this model to make $g$ closer to $f$?

scarcity,size, supply and demand,shinynes,Phosphorescent of the diamond, color of Phosphorescent.

Even if you allowed for so much expressivity in $\mathcal{H}$ that $f$ was an element in it, there would still be error due to ignorance of relevant information that you haven't measured. What information do you think can help? This is not a data science question - you have to think like someone who sells diamonds.

what color the person likes or shape, what reigon the diamond is from

9. Validate the model in #8 by reserving 10% of $\mathbb{D}$ as test data. Report oos standard error of the residuals

```{r}
n = nrow(diamonds)
K = 10
test_in=sample(1:n,size=n*1/K)
train_in=sample(setdiff(1:n,test_in))



#make sure we did this right:
pacman::p_load(testthat)
expect_equal(1 : n, sort(c(test_in, train_in)))

diamonds_train = diamonds[train_in, ]
diamonds_test = diamonds[test_in, ]

mod8p10=lm(price~.*.+poly(carat,5,raw=TRUE)+poly(x,5,raw=TRUE)+poly(y,5,raw=TRUE)+poly(z,5,raw=TRUE)+poly(depth,5,raw=TRUE)+poly(table,5,raw=TRUE),diamonds_train)
y_train=predict(mod8p10,diamonds_test)
y_test=diamonds_test$price
s_e_s=sd(y_train-y_test)
s_e_s
```

Compare the oos standard error of the residuals to the standard error of the residuals you got in #8 (i.e. the in-sample estimate). Do you think there's overfitting?

no

Extra-credit: validate the model via cross validation.

```{r}
#TO-DO if you want extra credit
```

Is this result much different than the single validation? And, again, is there overfitting in this model?

** TO-DO

10. The following code (from plec 14) produces a response that is the result of a linear model of one predictor and random $\epsilon$.

```{r}
rm(list = ls())
set.seed(1003)
n = 100
beta_0 = 1
beta_1 = 5
xmin = 0
xmax = 1
x = runif(n, xmin, xmax)
#best possible model
h_star_x = beta_0 + beta_1 * x

#actual data differs due to information we don't have
epsilon = rnorm(n)
y = h_star_x + epsilon
```

We then add fake predictors. For instance, here is the model with the addition of 2 fake predictors:

```{r}
p_fake = 2
X = matrix(c(x, rnorm(n * p_fake)), ncol = 1 + p_fake)
mod = lm(y ~ X)
```

Using a test set hold out, find the number of fake predictors where you can reliably say "I overfit". Some example code is below that you may want to use:

```{r}
tot=rep(NA,100)

for(i in 1:100){
  X=as.data.frame(matrix(c(x,rnorm(n*i)),ncol=i+1))
  K=10
  testind=sample(1:n,size=n*1/K)
  trainind=sample(setdiff(1:n,testind))
  train=X[trainind,]
  test=X[testind,]
  yhtrain=y[trainind]
  yhtest=y[testind]
  mod=lm(yhtrain~.,train)
  yhoss=predict(mod,test)
  osse=sd(yhtest-yhoss)
  tot[i]=osse
}
tot
```

it looks like around 75 predictors we start to overfitt