---
title: "Lecture 21 MATH 390.4 Queens College"
author: "Professor Adam Kapelner"
date: "April 30, 2018"
---

# Logistic Regression for Binary Response

Let's load up the cancer dataset, remove missing data, remove the ID column and add good names:

```{r}
biopsy = MASS::biopsy
biopsy$ID = NULL
biopsy = na.omit(biopsy)
colnames(biopsy) = c( #should've done this awhile ago!!!
  "clump_thickness",
  "cell_size_uniformity",
  "cell_shape_uniformity",
  "marginal_adhesion",
  "epithelial_cell_size",
  "bare_nuclei",
  "bland_chromatin",
  "normal_nucleoli",
  "mitoses",
  "class"
)
```

Let's code "malignant" as 1 and "benign" as 0.

```{r}
biopsy$class = ifelse(biopsy$class == "malignant", 1, 0)
```


Now let's split into training and test for experiments:

```{r}
test_prop = 0.2
train_indices = sample(1 : nrow(biopsy), round((1 - test_prop) * nrow(biopsy)))
biopsy_train = biopsy[train_indices, ]
y_train = biopsy_train$class
X_train = biopsy_train
X_train$class = NULL
test_indices = setdiff(1 : nrow(biopsy), train_indices)
biopsy_test = biopsy[test_indices, ]
y_test = biopsy_test$class
X_test = biopsy_test
X_test$class = NULL
```


Let's fit a linear logistic regression model. We use the function `glm` which looks a lot like `lm` except we have to pass in the parameter "binomial" which means we are using the independent Bernoulli. There are other types of models we won't get a chance to study e.g. Poisson, negative binomial.

```{r}
logistic_mod = glm(class ~ ., biopsy_train, family = "binomial")
```

Let's look at the $b$ vector that was made:

```{r}
coef(logistic_mod)
```

Interpretation? If clump thickness increases by one unit...

And let's take a look at the fitted values:

```{r}
head(predict(logistic_mod, biopsy_train))
```

What's that? Those are the "inverse link" values. In this case, they are log-odds of being malginant. If you can read log odds, you'll see ... has a small change of being malignant and ... has a high probability of being malignant. It's not that hard to read log odds...

What if we want probabilities? We can tell the predict function for `glm` to give us them explicitly:

```{r}
head(predict(logistic_mod, biopsy_train, type = "response"))
```

Let's take a look at all the in-sample probability estimates:

```{r}
p_hats_train = predict(logistic_mod, biopsy_train, type = "response")
pacman::p_load(ggplot2)
ggplot(data.frame(p_hats_train = p_hats_train, y_train = y_train)) + 
  geom_histogram(aes(x = p_hats_train, fill = factor(y_train)))
```

It's very sure of itself! 

Let's see $y$ by $\hat{p}$ another way:

```{r}
ggplot(data.frame(p_hats_train = p_hats_train, y_train = factor(y_train))) + 
  geom_boxplot(aes(x = y_train, y = p_hats_train))
```

Made only a few mistakes here and there...

What is the in-sample average Brier score?

```{r}
mean(-(y_train - p_hats_train)^2)
```

This is very good Brier score! Again, most of the probabilities were spot on.

But this was in sample! Let's see what happens out of sample..


```{r}
p_hats_test = predict(logistic_mod, biopsy_test, type = "response")
ggplot(data.frame(p_hats_test = p_hats_test, y_test = y_test)) + 
  geom_histogram(aes(x = p_hats_test, fill = factor(y_test)))
```

Still tends to be so sure of itself.

```{r}
ggplot(data.frame(p_hats_test = p_hats_test, y_test = factor(y_test))) + 
  geom_boxplot(aes(x = y_test, y = p_hats_test))
```

Not bad... and the Brier score?

```{r}
mean(-(y_test - p_hats_test)^2)
```

Not as good but still very good!

```{r}
rm(list = ls())
```

Let's try a harder project... load up the adult dataset where the response is 1 if the person makes more than \$50K per year and 0 if they make less than \$50K per year.

```{r}
pacman::p_load_gh("coatless/ucidata")
data(adult)
adult = na.omit(adult) #kill any observations with missingness
str(adult)
?adult
```

Let's use samples of 5,000 to run experiments:

```{r}
train_size = 5000
train_indices = sample(1 : nrow(adult), train_size)
adult_train = adult[train_indices, ]
y_train = adult_train$income
X_train = adult_train
X_train$income = NULL

test_size = 5000
test_indices = sample(setdiff(1 : nrow(adult), train_indices), test_size)
adult_test = adult[test_indices, ]
y_test = adult_test$income
X_test = adult_test
X_test$income = NULL
```

Let's fit a logistic regression model to the training data:

```{r}
logistic_mod = glm(income ~ ., adult_train, family = "binomial")
```

Numeric errors already!

Let's see what the model looks like:

```{r}
coef(logistic_mod)
```

There may be NA's above due to numeric errors. Usually happens if there is linear dependence (or near linear dependence). Interpretation?

Let's take a look at the fitted probability estimates:

```{r}
head(predict(logistic_mod, adult_train, type = "response"))
```

Let's take a look at all the in-sample probability estimates:

```{r}
p_hats_train = predict(logistic_mod, adult_train, type = "response")
pacman::p_load(ggplot2)
ggplot(data.frame(p_hats_train = p_hats_train, y_train = y_train)) + 
  geom_histogram(aes(x = p_hats_train, fill = factor(y_train)))
```

Much more humble!! It's not a very confident model since this task is much harder! In fact it's never confident about the large incomes and usually confident about the small incomes.

Let's see $y$ by $\hat{p}$:

```{r}
ggplot(data.frame(p_hats_train = p_hats_train, y_train = factor(y_train))) + 
  geom_boxplot(aes(x = y_train, y = p_hats_train))
```

Making lots of mistakes!

Note that the x-axis is the native category label since we never coded as 0, 1. The default is that the first label is 0 and the second is 1. The labels are defaulted to alphabetical order (I think...)

What is the in-sample average Brier score?

```{r}
mean(-(y_train - p_hats_train)^2)
```

Can't use factors here. Need to code the response as 0/1

```{r}
table(as.numeric(y_train)) #casting works... almost...
y_train_numeric = as.numeric(y_train) - 1
mean(-(y_train_numeric - p_hats_train)^2)
```

This is worse than the previous dataset but not terrible. The null model gives what?

```{r}
mean(-(y_train_numeric - rep(mean(y_train_numeric), length(y_train_numeric)))^2)
```

So this is a decent Brier score! Again, most of the probabilities were spot on.

But this was in sample! Let's see what happens out of sample..


```{r}
p_hats_test = predict(logistic_mod, adult_test, type = "response")
ggplot(data.frame(p_hats_test = p_hats_test, y_test = y_test)) + 
  geom_histogram(aes(x = p_hats_test, fill = factor(y_test)))
```

Looks similar to training. And the Brier score?

```{r}
y_test_numeric = as.numeric(y_test) - 1
mean(-(y_test_numeric - p_hats_test)^2)
```

The oos performance is about the same as the in-sample performance so we probably didn't overfit.

Brier scores only make sense if you know how to read Brier scores. It's kind of like learning a new language. However, everyon understands classification errors!
