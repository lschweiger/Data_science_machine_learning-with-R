---
title: "Lecture 9 MATH 390.4 Queens College"
author: "Professor Adam Kapelner"
date: "February 28, 2018"
---

Let's load up the cars data again.

```{r}
?MASS::Cars93
cars = MASS::Cars93
(cars)
dim(cars)
```

Let's take a look at the factor variable as the predictor. We'll use `Origin` in this example:

```{r}
table(cars$Origin)
```

We can plot how this looks:

```{r}
ggplot(cars, aes(Origin, Price)) + 
  geom_point() + 
  geom_smooth(method = 'lm')
```

Note that ggplot cannot handle fitting a line with a factor variable.

However...

```{r}
simple_linear_model = lm(Price ~ Origin, data = cars)
coef(simple_linear_model)
summary(simple_linear_model)$r.squared
summary(simple_linear_model)$sigma
```

What happened here? The `lm` function can handle factors. It picks a level to be the reference category (in this case, it's origin = USA) and the fitted slope $b_1$ would be the difference between non-USA and USA. Does it make sense that the slope is positive? Yes - foreign cars charge transport fees and there are a lot of luxury foreign cars.

Why is $R^2$ so low?? Remember the null model is one $\bar{y}$, this model is just two $\bar{y}$'s. How much variance can you possible expect to explain with just two possible prediction values?? Now take a look at RMSE. It's about \$10,000. Before, it was about \$6,000 and the $R^2$ = 62\%. Yes the RMSE has gone up in this case. $R^2$ is not proportion standard error explained, it's proportion variance and that squared error is very different than its square root.

Let's cast this predict as numeric and run it again just to make sure it will be the same. We first code a new dummy variable:

```{r}
cars$origin_is_not_usa = ifelse(cars$Origin == "non-USA", 1, 0)
```

and then we model using this new dummy variable:

```{r}
simple_linear_model = lm(Price ~ origin_is_not_usa, data = cars)
coef(simple_linear_model)
summary(simple_linear_model)$r.squared
summary(simple_linear_model)$sigma
```

Note the reference category is USA and the "non-USA" coefficient indicates the difference in sample averages.

Note that now ggplot can handle the line:

```{r}
ggplot(cars, aes(origin_is_not_usa, Price)) + 
  geom_point() + 
  geom_smooth(method = 'lm')
```

Let's code the dummy variable differently to take a look at the equivalent regression but this time with the reference category as non-USA

```{r}
cars$origin_is_usa = ifelse(cars$Origin == "USA", 1, 0)
```

```{r}
simple_linear_model = lm(Price ~ origin_is_usa, data = cars)
coef(simple_linear_model)
summary(simple_linear_model)$r.squared
summary(simple_linear_model)$sigma
```

The coefficients here are like "the opposite" in some sense of what we just saw. 

And of course $R^2$ and RMSE are equivalent - it's the same linear model with the same information, just coded differently.

The following is not covered right now. We may return to it later. I will leave out the intercept:

```{r}
simple_linear_model = lm(Price ~ 0 + Origin, data = cars)
coef(simple_linear_model)
summary(simple_linear_model)$r.squared
summary(simple_linear_model)$sigma
```

I think the `lm` method is calculating $R^2$ differently here if there is no intercept. I think the null model it is comparing to is $g = 0$ not $\bar{y}$.