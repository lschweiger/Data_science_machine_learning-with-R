---
title: "Lecture 5 MATH 390.4 Queens College"
author: "Professor Adam Kapelner"
date: "February 14, 2018"
---


## Computer Science Sidebar

Before we get back to modeling, it is worth knowing a couple more data structures in R. These are not "data [science] types", these are "[computer science] data types". 

The first are "lists" which are "ordered hashmaps" or "ordered dictionaries" or "hash tables". if You don't know what this is, you should read about this online as it should have been covered in a intro to CS class.

```{r}
dict = list()
dict$a = "first"
dict$b = "second"
dict$c = "third"
dict
length(dict)
names(dict) #keys
dict_unlisted = unlist(dict) #values
dict_unlisted
class(dict_unlisted) #i.e. a vector
#three ways to access values by key / ordered location
dict$a
dict[["a"]]
dict[[1]] #value of first entered key
class(dict[[1]])
dict[[1 : 2]] #bombs
#now let's try to access a value for a non-existent key / ordered location
dict$q
dict[["q"]]
dict[[4]] #bombs
#convenient means to subset the list
dict[1]
class(dict[1])
dict[1 : 2]
dict[1 : 4] #this is the reason this type of access is not recommended
dict = list("first", "second", "third") #no key => value... what happens?
dict #default keys are the numbers 1, 2, ...
dict[[1]]
dict = list("a" = "first", "b" = "second", "c" = "third") #key => value
dict
```

Lists conveniently allow all sorts of data types.

```{r}
varied_dict = list()
varied_dict$a = "first"
varied_dict$b = 2
varied_dict$c = 1 : 7
varied_dict$d = matrix(NA, nrow = 2, ncol = 2)
varied_dict[["some function"]] = function(x){x^2} #this key is not recommended
varied_dict
varied_dict$`some function` #note the tick marks (sometimes seen) needed due to spaces in key name
length(varied_dict)
names(varied_dict)
```

They have lots of uses in data science applications. We will likely see them in class and if not, you'll definitely see them in the real world. Note that data.frame objects are implemented as lists as well as many other common R objects.


The second is arrays i.e. are multidimensional vectors

```{r}
x = array(1 : 5, 5)
x
X = array(1 : 25, dim = c(5, 5))
X
X = array(1 : 125, dim = c(5, 5, 5))
X
X[1, , ]
X[, 1, ]
X[, , 1]
X[1, 1, 1]
```

These can be associative arrays too and operate like a hash of vectors across arbitrary dimensions:

```{r}
X = array(1 : 125, 
          dim = c(5, 5, 5),
          dimnames = list(
            c("A", "B", "C", "D", "E"),
            c("I", "II", "III", "IV", "V"),
            c("blue", "red", "green", "yellow", "orange")
          ))
X
X["A", , ]
X[, "III", ]
X[, , "orange"]
X["C", , "orange"]
X["C", "IV", "orange"]
```


## Saving and Loading in R

Let's save one object:

```{r}
save(varied_dict, file = "varied_dict.RData") #RData is recommended extension for convenience
rm(varied_dict)
varied_dict
load("varied_dict.RData")
varied_dict
```

Or save the whole workspace:

```{r}
save.image("all_work.RData")
rm(list = ls())
ls() #empty vector i.e. nothing here!
load("all_work.RData")
ls()
```

```{r}
rm(list = ls()) #cleanup everything
```


## Matrix operations in R

R can do all the standard matrix operations. Let's go through them quickly. First initialize two example matrices:

```{r}
A = matrix(rep(1, 4), nrow = 2)
A
B = array(seq(1, 4), dim = c(2, 2))
B
I = diag(2) #create an identity matrix of size 2x2
I
```

Now we show off some operations:

```{r}
A * B #element-wise multiplication
A %*% B #matrix multiplication
B %*% I
t(B) #transpose
solve(B)
solve(A) #BOOM - why?
solve(I)
#rank(A) = 1 #no such function... but... there are tons of add-on libraries for matrix computations e.g.
pacman::p_load(Matrix) #load the Matrix library
rankMatrix(B)
rankMatrix(A)
rankMatrix(I)
#technical note, skip
?attr #not recommended... return as lists!
```

Note that vectors and matrices are not the same:

```{r}
v = c(1, 2, 3) #3-d vector
t(v) #converts to 1x3 vector... unsure why
t(t(v))
v %*% v #seems to default to dot product
t(v) %*% t(t(v)) #dot product
I = diag(3)
I %*% v #seems to default correctly!
I %*% t(v) #actually uncomformable
```

We can also do eigen decomposition very easily:

```{r}
eigen_decomp = eigen(B)
V = eigen_decomp$vectors
v1 = V[, 1, drop = FALSE]
v2 = V[, 2, drop = FALSE]
lambdas = eigen_decomp$values
lambda1 = lambdas[1]
lambda2 = lambdas[2]

B %*% v1
lambda1 * v1

B %*% v2 
lambda2 * v2
B %*% v2 == lambda2 * v2 #why not?

B
V %*% diag(lambdas) %*% solve(V)
```

We'll return to eigenvalues/vectors at some point in the future. Hopefully when we do principal components analysis.


## The Perceptron

Time for some new data first... we are bored of the fabricated creditworthiness data.

```{r}
Xy = na.omit(MASS::biopsy) #The "breast cancer" data
?MASS::biopsy
head(Xy)
X = Xy[, 2 : 10] #V1, V2, ..., V9
head(X)
y_binary = as.numeric(Xy$class == "malignant")
table(y_binary)
```

First question. Let $\mathcal{H}$ be the set $\{0, 1\}$ meaning $g = 0$ or $g = 1$. What are the error rates then on $\mathbb{D}$? 

```{r}
#If always 0, all the 1's are errors
239 / (444 + 239)
#If always 1, all the 0's are errors
444 / (444 + 239)
```

If your $g$ can't beat that, either your features $x_1, \ldots, x_p$ are terrible, and/or $\mathcal{H}$ was a terrible choice and/or $\mathcal{A}$ can't pull its weight.

Okay... back to the "perceptron learning algorithm".

Let's do so for one dimension - just "V1" in the breast cancer data. You will do an example with more features for homework.

```{r}
y_binary = ifelse(y_binary == 1, 0, 1)
MAX_ITER = 1000
w_vec = rep(0, 2) #intialize a 2-dim vector

X1 = as.matrix(cbind(1, X[, 1, drop = FALSE]))

for (iter in 1 : MAX_ITER){  
  for (i in 1 : nrow(X1)){
    x_i = X1[i, ]
    yhat_i = ifelse(sum(x_i * w_vec) > 0, 1, 0)
    y_i = y_binary[i]
    w_vec = w_vec + (y_i - yhat_i) * x_i
  }
}
w_vec
```

What is our error rate?

```{r}
yhat = ifelse(X1 %*% w_vec > 0, 1, 0)
sum(y_binary != yhat) / length(y_binary)
```

## The linear threshold model

Using the algorithm $\mathcal{A}$ discussed previously, this model was dubbed the "perceptron". However, there are othe algorithms $\mathcal{A}$ we can use to fit the "best" model in $\mathcal{H}$.

We spoke about how the `w` vector is in a large space, $R^p$. A course in optimization will describe methods how to find optimal and approximately optimal solutions for `w` based on an "objective function" or "fitness function" or "cost function". Here that target is the number of total errors of the `n` examples in the training data set.

```{r}
SAE = function(w_vec){
  yhat = ifelse(X1 %*% w_vec > 0, 1, 0)
  sum(y_binary != yhat)
}
```


Here are some off-the-shelf solvers in R in action. Note: this is not the classic "perceptron learning algorithm". Also: this is complete junk made-up data so it's only an illustration. You will see real data for your homework.

The first is `nlm` which uses some sort of Newton-like algorithm.

```{r}
?nlm
w_vec = nlm(SAE, c(1, 1))$estimate #doesn't work at all ... I think because our cost function is not continuous
w_vec
```
That didn't seem to work at all!

What is our error rate?

```{r}
yhat = ifelse(X1 %*% w_vec > 0, 1, 0)
sum(y_binary != yhat) / length(y_binary)
```

Garbage... because it didn't work. Anyone know why?

Let's try another optimization algorithm called Nelder-Mead developed by John Nelder and Roger Mead (both were famous statisticians) in 1965.

```{r}
pacman::p_load(neldermead)
?optim
optim_output = optim(c(0, 0), SAE)
optim_output
w_vec = optim_output$par
```

What is our error rate using the Nelder-Mead local optimum?

```{r}
yhat = ifelse(X1 %*% w_vec > 0, 1, 0)
sum(y_binary != yhat) / length(y_binary)
```

This beats the perceptron! 

Lesson: perceptron learning algorithm maybe not so great. Also - different $\mathcal{A}$'s give you wildly different models $g$ even with the same allowable functions provided in $\mathcal{H}$. 

Most of the really creative work is within $\mathcal{A}$ although specifying $\mathcal{H}$ suitably flexible is very important too.
