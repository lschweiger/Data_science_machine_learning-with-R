---
title: "Lecture 15 MATH 390.4 Queens College"
author: "Professor Adam Kapelner"
date: "March 28, 2018"
---

# Assessing overfitting in practice

Let's examine this again. This time we use one data set which is split between training and testing.

```{r}
set.seed(1003)
n = 100
beta_0 = 1
beta_1 = 5
xmin = 0
xmax = 1
p = 50
X = matrix(runif(n * p, xmin, xmax), ncol = p)

#best possible model - only one predictor matters!
h_star_x = beta_0 + beta_1 * X[,1 ]

#actual data differs due to information we don't have
epsilon = rnorm(n)
y = h_star_x + epsilon
```

Now we split $\mathbb{D}$ into training and testing. We define $K$ first, the inverse proportion of the test size.

```{r}
K = 5 #i.e. the test set is 1/5th of the entire historical dataset

#a simple algorithm to do this is to sample indices directly
test_indices = sample(1 : n, 1 / K * n)
train_indices = setdiff(1 : n, test_indices)

#now pull out the matrices and vectors based on the indices
X_train = X[train_indices, ]
y_train = y[train_indices]
X_test = X[test_indices, ]
y_test = y[test_indices]

#let's ensure these are all correct
dim(X_train)
dim(X_test)
length(y_train)
length(y_test)
```

Now let's fit the model $g$ to the training data and compute in-sample error metrics:

```{r}
mod = lm(y_train ~ ., data.frame(X_train))
summary(mod)$r.squared
sd(mod$residuals)
```

Now let's see how we do on the test data. We compute $R^2$ and $s_e$ out of sample:

```{r}
y_hat_oos = predict(mod, data.frame(X_test))
oos_residuals = y_test - y_hat_oos
1 - sum(oos_residuals^2) / sum((y_test - mean(y_test))^2)
sd(oos_residuals)
```

MUCH worse!! Why? We overfit big time...

Can we go back now and fit a new model and see how we did? NO...

So how are we supposed to fix a "bad" model? We can't unless we do something smarter. We'll get there.

# Nonlinear Linear Regression: Polynomial Regression

Let's generate a polynomial model of degree 2 ($f = h^* \in \mathcal{H}$) and let $\epsilon$ be random noise (the error due to ignorance) for $\mathbb{D}$ featuring $n = 2$.

```{r}
set.seed(1003)
n = 25
beta_0 = 1
beta_1 = 0
beta_2 = 1
x = runif(n, -2, 5)
#best possible model
h_star_x = beta_0 + beta_1 * x + beta_2 * x^2

#actual data differs due to information we don't have
epsilon = rnorm(n)
y = h_star_x + epsilon

#scatterplot it
df = data.frame(x = x, y = y, h_star_x = h_star_x)
basic = ggplot(df, aes(x, y)) +
  geom_point()
basic
```

Let's try to estimate with a line:

```{r}
linear_mod = lm(y ~ x)
b_linear = summary(linear_mod)$coef
basic + geom_abline(intercept = b_linear[1], slope = b_linear[2], col = "red")
```

The relationship is "underfit". $\mathcal{H}$ is not rich enough right now to express something close to $f(x)$. But it is better than the null model!

Now let's do a polynomial regression of degree two. Let's do so manually:

```{r}
X = as.matrix(cbind(1, x, x^2))
b = solve(t(X) %*% X) %*% t(X) %*% y
b
```

These are about the same as the $\beta_0, \beta_1$ and $\beta_2$ as defined in $f(x)$ the true model. In order to graph this, we can no longer use the routine `geom_abline`, we need to use `stat_function`.

```{r}
plot_function_degree_2 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2
}

basic + stat_function(fun = plot_function_degree_2, args = list(b = b), col= "darkgreen")
```

Now let's try polynomial of degree 3:

```{r}
X = as.matrix(cbind(1, x, x^2, x^3))
b = solve(t(X) %*% X) %*% t(X) %*% y
b

plot_function_degree_3 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x^3
}

basic + stat_function(fun = plot_function_degree_3, args = list(b = b), col= "darkgreen")
```
Still the same. Why? The $x^3$ term is like adding one "nonsense" predictor. One nonsense predictor marginally affects $R^2$ but it doesn't matter too much.

Now let's try polynomial of degree 8:

```{r}
X = as.matrix(cbind(1, x, x^2, x^3, x^4, x^5, x^6, x^7, x^8))
b = solve(t(X) %*% X) %*% t(X) %*% y
b

plot_function_degree_8 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x^3 + b[5] * x^4 + b[6] * x^5 + b[7] * x^6 + b[8] * x^7 + b[9] * x^8 
}

basic + stat_function(fun = plot_function_degree_8, args = list(b = b), col= "darkgreen")
```

We are seeing now a little bit of "overfitting" in the edge(s). We now have $p=11$ and $n=100$. We can do a lot worse!

Let's learn how to do this in R first without having to resort to manual linear algebra. R has a function called "poly" that can be used *inside* formula declarations.

Let's first fit the degree 2 model:

```{r}
degree_2_poly_mod = lm(y ~ poly(x, 2, raw = TRUE))
b_poly_2 = coef(degree_2_poly_mod)
b_poly_2
```

Same as we got before! We use "raw" polynomials to keep them interpretable and on the same scale as the manual models we were fitting.

Now let's do polynomial of degree 13:

```{r}
degree_13_poly_mod = lm(y ~ poly(x, 13, raw = TRUE))
b_poly_13 = coef(degree_13_poly_mod)

plot_function_degree_13 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x^3 + b[5] * x^4 + b[6] * x^5 + b[7] * x^6 + b[8] * x^7 + b[9] * x^8 + b[10] * x^9  + b[11] * x^10 + b[12] * x^11 + b[13] * x^12 + b[14] * x^13
}

basic + stat_function(fun = plot_function_degree_13, args = list(b = b_poly_13), col = "purple")
```

What's happening for small values of $x$ (and a bit for large values)? This is called [Runge's Phenomenon](https://en.wikipedia.org/wiki/Runge%27s_phenomenon) meaning that the boundary activity of high-order polynomials has very large derivatives. Let's go back to the same scale as before and see what's happening:

```{r}
basic + 
  coord_cartesian(xlim = c(-2, 5), ylim = c(-3, 25)) + 
  stat_function(fun = plot_function_degree_13, args = list(b = b_poly_13), col = "purple")
```


What happens during extrapolation? Let's look at the (a) linear model, (b) polynomial model with degree 2 and (c) polynomial with degree 13.

```{r}
xmin = -10
xmax = 8
basic + 
  coord_cartesian(xlim = c(xmin, xmax), ylim = c(-10, 35)) + 
  geom_abline(intercept = b_linear[1], slope = b_linear[2], col = "red") + 
  stat_function(fun = plot_function_degree_2, args = list(b = b_poly_2), col = "blue", xlim = c(xmin, xmax)) +
  stat_function(fun = plot_function_degree_13, args = list(b = b_poly_13), col = "purple", xlim = c(xmin, xmax))
```

Polynomial models have *TERRIBLE* extrapolation ability - totally unpredictable.

Can we achieve $R^2 = 100\%$ using polynomial regression? Yes. Here's an example in one dimension. These are called "interpolation polynomials". In one dimension, as long as the $x$ values are distinct, $n$ data point can be fit by a $n - 1$ degree polynomial. Here's an example with a few data points:

```{r}
set.seed(1003)
n = 5
beta_0 = 1
beta_1 = 0
beta_2 = 1
x = runif(n)
y = runif(n)

#scatterplot it
df = data.frame(x = x, y = y, h_star_x = h_star_x)
basic = ggplot(df, aes(x, y)) +
  geom_point()
basic
```

Now fit polynomial models:

```{r}
degree_4_poly_mod = lm(y ~ poly(x, 4, raw = TRUE))
b_poly_4 = coef(degree_4_poly_mod)

plot_function_degree_4 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x^3 + b[5] * x^4
}

basic + stat_function(fun = plot_function_degree_4, args = list(b = b_poly_4), col = "purple")
```

Perfect fit!

```{r}
summary(degree_4_poly_mod)$r.squared
```

This is the same thing we've seen before! If $n = p + 1$, then the design matrix is square and there is no need to project onto a lower dimensional subspace. To estimate the linear model, one only needs to solve $n$ equations with $n$ unknowns.

My recommendations:
1) Keep polynomial degree low. Preferably 2. Anything past 2 is not interpretable anyway. We didn't talk about "interpretability" of models yet, but you get the idea.
2) Be very careful not to extrapolate: make sure future predictions have the measurements within range of the training data $\mathbb{D}$. Extrapolations are going to be very, very inaccurate. Polynomial regressions I'm sure have gotten data scientists fired before.

